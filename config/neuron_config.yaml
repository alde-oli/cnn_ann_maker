layer1:
  amount: 128 # Number of neurons in the layer (e.g., 128, 256, 512)
  weight_initialization: "xavier" # options: "random", "xavier", "he"
  bias_initialization: 0.0 # Initial bias value (e.g., 0.0, 0.1)

  activation_function:
    type: "sigmoid" # options: "sigmoid", "relu", "leaky_relu", "tanh", "elu", "softmax"
    leaky_relu_alpha: 0.01 # (Leaky ReLU only)

  error_function: "cross_entropy" # options: "mean_squared_error", "cross_entropy"

  gradient_descent:
    method: "adam" # options: "sgd", "adam", "rmsprop", "momentum"
    learning_rate: 0.001 # (e.g., 0.01, 0.001, 0.0001)
    momentum: 0.9 # Momentum factor (Momentum only)
    beta1: 0.9 # (Adam only)
    beta2: 0.999 # (Adam only)
    epsilon: 1e-7 # (Adam only)

  regularization:
    l1: 0.0 # L1 regularization factor (e.g., 0.0, 0.01)
    l2: 0.01 # L2 regularization factor (e.g., 0.0, 0.01)
  #  dropout_rate: 0.5 # Dropout rate (e.g., 0.0, 0.5)


layer2:
  amount: 64
  weight_initialization: "xavier"
  bias_initialization: 0.0

  activation_function:
    type: "relu"
    leaky_relu_alpha: 0.01
  
  error_function: "cross_entropy"

  gradient_descent:
    method: "adam"
    learning_rate: 0.001
    momentum: 0.9
    beta1: 0.9
    beta2: 0.999
    epsilon: 1e-7

  regularization:
    l1: 0.0
    l2: 0.01
  #  dropout_rate: 0.5

# add more layers as needed

training:
  epochs: 100 # Number of training epochs (e.g., 10, 100, 1000)
  batch_size: 32 # Number of samples per batch (e.g., 16, 32, 64)
